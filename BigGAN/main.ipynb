{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP advance course. HW3. Distributive semantic. Embedings_+_Bidirectional_GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GRh7ig80jk3s",
        "1_nWe7O-jqak",
        "TGzxJch3juGy",
        "sWmhQEFAj1VE",
        "_yDXCBmqtkPl",
        "0hTlPvNY8gi1"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "384px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f0861523d94e4b9d9ce9d79ef6473755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_64565fdc2a104d078fa71080b85538fe",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_47817ee2a74a42c3a8fc7204c0365f07",
              "IPY_MODEL_b341646c4d5149f1aae165d71fc16340"
            ]
          }
        },
        "64565fdc2a104d078fa71080b85538fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "47817ee2a74a42c3a8fc7204c0365f07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1604364b009f406a8833847cbc30a22a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 159571,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 159571,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4fcaa74f5b29413bb2178c8520dd9f47"
          }
        },
        "b341646c4d5149f1aae165d71fc16340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_113ef6ba71ce478e9d9e76f25c2c80ed",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 159571/159571 [02:10&lt;00:00, 1222.42it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c58927dabebb4816a2867a235c954809"
          }
        },
        "1604364b009f406a8833847cbc30a22a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4fcaa74f5b29413bb2178c8520dd9f47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "113ef6ba71ce478e9d9e76f25c2c80ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c58927dabebb4816a2867a235c954809": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "df8015745cbd4c30a12dfd44b273c963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_aa10d009e0ae4071bed574aa48f8771e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_20c9965e717c4089b4b4ad63952c0a1b",
              "IPY_MODEL_4b64050383b745118540aab5408979cd"
            ]
          }
        },
        "aa10d009e0ae4071bed574aa48f8771e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "20c9965e717c4089b4b4ad63952c0a1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e7ee05fa61b84b99b31beab29f4a8407",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 153164,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 153164,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cdf14fcb841c473f957636d853ddaa5e"
          }
        },
        "4b64050383b745118540aab5408979cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_adffb40a3a9046929e59624559c6e72c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 153164/153164 [02:00&lt;00:00, 1266.43it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cf3560801f8b41f0a45d2c475c7a2724"
          }
        },
        "e7ee05fa61b84b99b31beab29f4a8407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cdf14fcb841c473f957636d853ddaa5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "adffb40a3a9046929e59624559c6e72c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cf3560801f8b41f0a45d2c475c7a2724": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OlegBEZb/MLHack_non_copyrighted/blob/master/BigGAN/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwhsyrEA3hlu",
        "colab_type": "text"
      },
      "source": [
        "#Imagenet classes mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sOFWT4Z3hKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imagenet_classes_mapping = {0: 'tench, Tinca tinca',\n",
        " 1: 'goldfish, Carassius auratus',\n",
        " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
        " 3: 'tiger shark, Galeocerdo cuvieri',\n",
        " 4: 'hammerhead, hammerhead shark',\n",
        " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
        " 6: 'stingray',\n",
        " 7: 'cock',\n",
        " 8: 'hen',\n",
        " 9: 'ostrich, Struthio camelus',\n",
        " 10: 'brambling, Fringilla montifringilla',\n",
        " 11: 'goldfinch, Carduelis carduelis',\n",
        " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
        " 13: 'junco, snowbird',\n",
        " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
        " 15: 'robin, American robin, Turdus migratorius',\n",
        " 16: 'bulbul',\n",
        " 17: 'jay',\n",
        " 18: 'magpie',\n",
        " 19: 'chickadee',\n",
        " 20: 'water ouzel, dipper',\n",
        " 21: 'kite',\n",
        " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
        " 23: 'vulture',\n",
        " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
        " 25: 'European fire salamander, Salamandra salamandra',\n",
        " 26: 'common newt, Triturus vulgaris',\n",
        " 27: 'eft',\n",
        " 28: 'spotted salamander, Ambystoma maculatum',\n",
        " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
        " 30: 'bullfrog, Rana catesbeiana',\n",
        " 31: 'tree frog, tree-frog',\n",
        " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
        " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
        " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
        " 35: 'mud turtle',\n",
        " 36: 'terrapin',\n",
        " 37: 'box turtle, box tortoise',\n",
        " 38: 'banded gecko',\n",
        " 39: 'common iguana, iguana, Iguana iguana',\n",
        " 40: 'American chameleon, anole, Anolis carolinensis',\n",
        " 41: 'whiptail, whiptail lizard',\n",
        " 42: 'agama',\n",
        " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
        " 44: 'alligator lizard',\n",
        " 45: 'Gila monster, Heloderma suspectum',\n",
        " 46: 'green lizard, Lacerta viridis',\n",
        " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
        " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
        " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
        " 50: 'American alligator, Alligator mississipiensis',\n",
        " 51: 'triceratops',\n",
        " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
        " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
        " 54: 'hognose snake, puff adder, sand viper',\n",
        " 55: 'green snake, grass snake',\n",
        " 56: 'king snake, kingsnake',\n",
        " 57: 'garter snake, grass snake',\n",
        " 58: 'water snake',\n",
        " 59: 'vine snake',\n",
        " 60: 'night snake, Hypsiglena torquata',\n",
        " 61: 'boa constrictor, Constrictor constrictor',\n",
        " 62: 'rock python, rock snake, Python sebae',\n",
        " 63: 'Indian cobra, Naja naja',\n",
        " 64: 'green mamba',\n",
        " 65: 'sea snake',\n",
        " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
        " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
        " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
        " 69: 'trilobite',\n",
        " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
        " 71: 'scorpion',\n",
        " 72: 'black and gold garden spider, Argiope aurantia',\n",
        " 73: 'barn spider, Araneus cavaticus',\n",
        " 74: 'garden spider, Aranea diademata',\n",
        " 75: 'black widow, Latrodectus mactans',\n",
        " 76: 'tarantula',\n",
        " 77: 'wolf spider, hunting spider',\n",
        " 78: 'tick',\n",
        " 79: 'centipede',\n",
        " 80: 'black grouse',\n",
        " 81: 'ptarmigan',\n",
        " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
        " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
        " 84: 'peacock',\n",
        " 85: 'quail',\n",
        " 86: 'partridge',\n",
        " 87: 'African grey, African gray, Psittacus erithacus',\n",
        " 88: 'macaw',\n",
        " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
        " 90: 'lorikeet',\n",
        " 91: 'coucal',\n",
        " 92: 'bee eater',\n",
        " 93: 'hornbill',\n",
        " 94: 'hummingbird',\n",
        " 95: 'jacamar',\n",
        " 96: 'toucan',\n",
        " 97: 'drake',\n",
        " 98: 'red-breasted merganser, Mergus serrator',\n",
        " 99: 'goose',\n",
        " 100: 'black swan, Cygnus atratus',\n",
        " 101: 'tusker',\n",
        " 102: 'echidna, spiny anteater, anteater',\n",
        " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
        " 104: 'wallaby, brush kangaroo',\n",
        " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
        " 106: 'wombat',\n",
        " 107: 'jellyfish',\n",
        " 108: 'sea anemone, anemone',\n",
        " 109: 'brain coral',\n",
        " 110: 'flatworm, platyhelminth',\n",
        " 111: 'nematode, nematode worm, roundworm',\n",
        " 112: 'conch',\n",
        " 113: 'snail',\n",
        " 114: 'slug',\n",
        " 115: 'sea slug, nudibranch',\n",
        " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
        " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
        " 118: 'Dungeness crab, Cancer magister',\n",
        " 119: 'rock crab, Cancer irroratus',\n",
        " 120: 'fiddler crab',\n",
        " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
        " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
        " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
        " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
        " 125: 'hermit crab',\n",
        " 126: 'isopod',\n",
        " 127: 'white stork, Ciconia ciconia',\n",
        " 128: 'black stork, Ciconia nigra',\n",
        " 129: 'spoonbill',\n",
        " 130: 'flamingo',\n",
        " 131: 'little blue heron, Egretta caerulea',\n",
        " 132: 'American egret, great white heron, Egretta albus',\n",
        " 133: 'bittern',\n",
        " 134: 'crane',\n",
        " 135: 'limpkin, Aramus pictus',\n",
        " 136: 'European gallinule, Porphyrio porphyrio',\n",
        " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
        " 138: 'bustard',\n",
        " 139: 'ruddy turnstone, Arenaria interpres',\n",
        " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
        " 141: 'redshank, Tringa totanus',\n",
        " 142: 'dowitcher',\n",
        " 143: 'oystercatcher, oyster catcher',\n",
        " 144: 'pelican',\n",
        " 145: 'king penguin, Aptenodytes patagonica',\n",
        " 146: 'albatross, mollymawk',\n",
        " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
        " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
        " 149: 'dugong, Dugong dugon',\n",
        " 150: 'sea lion',\n",
        " 151: 'Chihuahua',\n",
        " 152: 'Japanese spaniel',\n",
        " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
        " 154: 'Pekinese, Pekingese, Peke',\n",
        " 155: 'Shih-Tzu',\n",
        " 156: 'Blenheim spaniel',\n",
        " 157: 'papillon',\n",
        " 158: 'toy terrier',\n",
        " 159: 'Rhodesian ridgeback',\n",
        " 160: 'Afghan hound, Afghan',\n",
        " 161: 'basset, basset hound',\n",
        " 162: 'beagle',\n",
        " 163: 'bloodhound, sleuthhound',\n",
        " 164: 'bluetick',\n",
        " 165: 'black-and-tan coonhound',\n",
        " 166: 'Walker hound, Walker foxhound',\n",
        " 167: 'English foxhound',\n",
        " 168: 'redbone',\n",
        " 169: 'borzoi, Russian wolfhound',\n",
        " 170: 'Irish wolfhound',\n",
        " 171: 'Italian greyhound',\n",
        " 172: 'whippet',\n",
        " 173: 'Ibizan hound, Ibizan Podenco',\n",
        " 174: 'Norwegian elkhound, elkhound',\n",
        " 175: 'otterhound, otter hound',\n",
        " 176: 'Saluki, gazelle hound',\n",
        " 177: 'Scottish deerhound, deerhound',\n",
        " 178: 'Weimaraner',\n",
        " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
        " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
        " 181: 'Bedlington terrier',\n",
        " 182: 'Border terrier',\n",
        " 183: 'Kerry blue terrier',\n",
        " 184: 'Irish terrier',\n",
        " 185: 'Norfolk terrier',\n",
        " 186: 'Norwich terrier',\n",
        " 187: 'Yorkshire terrier',\n",
        " 188: 'wire-haired fox terrier',\n",
        " 189: 'Lakeland terrier',\n",
        " 190: 'Sealyham terrier, Sealyham',\n",
        " 191: 'Airedale, Airedale terrier',\n",
        " 192: 'cairn, cairn terrier',\n",
        " 193: 'Australian terrier',\n",
        " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
        " 195: 'Boston bull, Boston terrier',\n",
        " 196: 'miniature schnauzer',\n",
        " 197: 'giant schnauzer',\n",
        " 198: 'standard schnauzer',\n",
        " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
        " 200: 'Tibetan terrier, chrysanthemum dog',\n",
        " 201: 'silky terrier, Sydney silky',\n",
        " 202: 'soft-coated wheaten terrier',\n",
        " 203: 'West Highland white terrier',\n",
        " 204: 'Lhasa, Lhasa apso',\n",
        " 205: 'flat-coated retriever',\n",
        " 206: 'curly-coated retriever',\n",
        " 207: 'golden retriever',\n",
        " 208: 'Labrador retriever',\n",
        " 209: 'Chesapeake Bay retriever',\n",
        " 210: 'German short-haired pointer',\n",
        " 211: 'vizsla, Hungarian pointer',\n",
        " 212: 'English setter',\n",
        " 213: 'Irish setter, red setter',\n",
        " 214: 'Gordon setter',\n",
        " 215: 'Brittany spaniel',\n",
        " 216: 'clumber, clumber spaniel',\n",
        " 217: 'English springer, English springer spaniel',\n",
        " 218: 'Welsh springer spaniel',\n",
        " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
        " 220: 'Sussex spaniel',\n",
        " 221: 'Irish water spaniel',\n",
        " 222: 'kuvasz',\n",
        " 223: 'schipperke',\n",
        " 224: 'groenendael',\n",
        " 225: 'malinois',\n",
        " 226: 'briard',\n",
        " 227: 'kelpie',\n",
        " 228: 'komondor',\n",
        " 229: 'Old English sheepdog, bobtail',\n",
        " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
        " 231: 'collie',\n",
        " 232: 'Border collie',\n",
        " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
        " 234: 'Rottweiler',\n",
        " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
        " 236: 'Doberman, Doberman pinscher',\n",
        " 237: 'miniature pinscher',\n",
        " 238: 'Greater Swiss Mountain dog',\n",
        " 239: 'Bernese mountain dog',\n",
        " 240: 'Appenzeller',\n",
        " 241: 'EntleBucher',\n",
        " 242: 'boxer',\n",
        " 243: 'bull mastiff',\n",
        " 244: 'Tibetan mastiff',\n",
        " 245: 'French bulldog',\n",
        " 246: 'Great Dane',\n",
        " 247: 'Saint Bernard, St Bernard',\n",
        " 248: 'Eskimo dog, husky',\n",
        " 249: 'malamute, malemute, Alaskan malamute',\n",
        " 250: 'Siberian husky',\n",
        " 251: 'dalmatian, coach dog, carriage dog',\n",
        " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
        " 253: 'basenji',\n",
        " 254: 'pug, pug-dog',\n",
        " 255: 'Leonberg',\n",
        " 256: 'Newfoundland, Newfoundland dog',\n",
        " 257: 'Great Pyrenees',\n",
        " 258: 'Samoyed, Samoyede',\n",
        " 259: 'Pomeranian',\n",
        " 260: 'chow, chow chow',\n",
        " 261: 'keeshond',\n",
        " 262: 'Brabancon griffon',\n",
        " 263: 'Pembroke, Pembroke Welsh corgi',\n",
        " 264: 'Cardigan, Cardigan Welsh corgi',\n",
        " 265: 'toy poodle',\n",
        " 266: 'miniature poodle',\n",
        " 267: 'standard poodle',\n",
        " 268: 'Mexican hairless',\n",
        " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
        " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
        " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
        " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
        " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
        " 274: 'dhole, Cuon alpinus',\n",
        " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
        " 276: 'hyena, hyaena',\n",
        " 277: 'red fox, Vulpes vulpes',\n",
        " 278: 'kit fox, Vulpes macrotis',\n",
        " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
        " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
        " 281: 'tabby, tabby cat',\n",
        " 282: 'tiger cat',\n",
        " 283: 'Persian cat',\n",
        " 284: 'Siamese cat, Siamese',\n",
        " 285: 'Egyptian cat',\n",
        " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
        " 287: 'lynx, catamount',\n",
        " 288: 'leopard, Panthera pardus',\n",
        " 289: 'snow leopard, ounce, Panthera uncia',\n",
        " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
        " 291: 'lion, king of beasts, Panthera leo',\n",
        " 292: 'tiger, Panthera tigris',\n",
        " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
        " 294: 'brown bear, bruin, Ursus arctos',\n",
        " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
        " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
        " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
        " 298: 'mongoose',\n",
        " 299: 'meerkat, mierkat',\n",
        " 300: 'tiger beetle',\n",
        " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
        " 302: 'ground beetle, carabid beetle',\n",
        " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
        " 304: 'leaf beetle, chrysomelid',\n",
        " 305: 'dung beetle',\n",
        " 306: 'rhinoceros beetle',\n",
        " 307: 'weevil',\n",
        " 308: 'fly',\n",
        " 309: 'bee',\n",
        " 310: 'ant, emmet, pismire',\n",
        " 311: 'grasshopper, hopper',\n",
        " 312: 'cricket',\n",
        " 313: 'walking stick, walkingstick, stick insect',\n",
        " 314: 'cockroach, roach',\n",
        " 315: 'mantis, mantid',\n",
        " 316: 'cicada, cicala',\n",
        " 317: 'leafhopper',\n",
        " 318: 'lacewing, lacewing fly',\n",
        " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
        " 320: 'damselfly',\n",
        " 321: 'admiral',\n",
        " 322: 'ringlet, ringlet butterfly',\n",
        " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
        " 324: 'cabbage butterfly',\n",
        " 325: 'sulphur butterfly, sulfur butterfly',\n",
        " 326: 'lycaenid, lycaenid butterfly',\n",
        " 327: 'starfish, sea star',\n",
        " 328: 'sea urchin',\n",
        " 329: 'sea cucumber, holothurian',\n",
        " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
        " 331: 'hare',\n",
        " 332: 'Angora, Angora rabbit',\n",
        " 333: 'hamster',\n",
        " 334: 'porcupine, hedgehog',\n",
        " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
        " 336: 'marmot',\n",
        " 337: 'beaver',\n",
        " 338: 'guinea pig, Cavia cobaya',\n",
        " 339: 'sorrel',\n",
        " 340: 'zebra',\n",
        " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
        " 342: 'wild boar, boar, Sus scrofa',\n",
        " 343: 'warthog',\n",
        " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
        " 345: 'ox',\n",
        " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
        " 347: 'bison',\n",
        " 348: 'ram, tup',\n",
        " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
        " 350: 'ibex, Capra ibex',\n",
        " 351: 'hartebeest',\n",
        " 352: 'impala, Aepyceros melampus',\n",
        " 353: 'gazelle',\n",
        " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
        " 355: 'llama',\n",
        " 356: 'weasel',\n",
        " 357: 'mink',\n",
        " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
        " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
        " 360: 'otter',\n",
        " 361: 'skunk, polecat, wood pussy',\n",
        " 362: 'badger',\n",
        " 363: 'armadillo',\n",
        " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
        " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
        " 366: 'gorilla, Gorilla gorilla',\n",
        " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
        " 368: 'gibbon, Hylobates lar',\n",
        " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
        " 370: 'guenon, guenon monkey',\n",
        " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
        " 372: 'baboon',\n",
        " 373: 'macaque',\n",
        " 374: 'langur',\n",
        " 375: 'colobus, colobus monkey',\n",
        " 376: 'proboscis monkey, Nasalis larvatus',\n",
        " 377: 'marmoset',\n",
        " 378: 'capuchin, ringtail, Cebus capucinus',\n",
        " 379: 'howler monkey, howler',\n",
        " 380: 'titi, titi monkey',\n",
        " 381: 'spider monkey, Ateles geoffroyi',\n",
        " 382: 'squirrel monkey, Saimiri sciureus',\n",
        " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
        " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
        " 385: 'Indian elephant, Elephas maximus',\n",
        " 386: 'African elephant, Loxodonta africana',\n",
        " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
        " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
        " 389: 'barracouta, snoek',\n",
        " 390: 'eel',\n",
        " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
        " 392: 'rock beauty, Holocanthus tricolor',\n",
        " 393: 'anemone fish',\n",
        " 394: 'sturgeon',\n",
        " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
        " 396: 'lionfish',\n",
        " 397: 'puffer, pufferfish, blowfish, globefish',\n",
        " 398: 'abacus',\n",
        " 399: 'abaya',\n",
        " 400: \"academic gown, academic robe, judge's robe\",\n",
        " 401: 'accordion, piano accordion, squeeze box',\n",
        " 402: 'acoustic guitar',\n",
        " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
        " 404: 'airliner',\n",
        " 405: 'airship, dirigible',\n",
        " 406: 'altar',\n",
        " 407: 'ambulance',\n",
        " 408: 'amphibian, amphibious vehicle',\n",
        " 409: 'analog clock',\n",
        " 410: 'apiary, bee house',\n",
        " 411: 'apron',\n",
        " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
        " 413: 'assault rifle, assault gun',\n",
        " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
        " 415: 'bakery, bakeshop, bakehouse',\n",
        " 416: 'balance beam, beam',\n",
        " 417: 'balloon',\n",
        " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
        " 419: 'Band Aid',\n",
        " 420: 'banjo',\n",
        " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
        " 422: 'barbell',\n",
        " 423: 'barber chair',\n",
        " 424: 'barbershop',\n",
        " 425: 'barn',\n",
        " 426: 'barometer',\n",
        " 427: 'barrel, cask',\n",
        " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
        " 429: 'baseball',\n",
        " 430: 'basketball',\n",
        " 431: 'bassinet',\n",
        " 432: 'bassoon',\n",
        " 433: 'bathing cap, swimming cap',\n",
        " 434: 'bath towel',\n",
        " 435: 'bathtub, bathing tub, bath, tub',\n",
        " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
        " 437: 'beacon, lighthouse, beacon light, pharos',\n",
        " 438: 'beaker',\n",
        " 439: 'bearskin, busby, shako',\n",
        " 440: 'beer bottle',\n",
        " 441: 'beer glass',\n",
        " 442: 'bell cote, bell cot',\n",
        " 443: 'bib',\n",
        " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
        " 445: 'bikini, two-piece',\n",
        " 446: 'binder, ring-binder',\n",
        " 447: 'binoculars, field glasses, opera glasses',\n",
        " 448: 'birdhouse',\n",
        " 449: 'boathouse',\n",
        " 450: 'bobsled, bobsleigh, bob',\n",
        " 451: 'bolo tie, bolo, bola tie, bola',\n",
        " 452: 'bonnet, poke bonnet',\n",
        " 453: 'bookcase',\n",
        " 454: 'bookshop, bookstore, bookstall',\n",
        " 455: 'bottlecap',\n",
        " 456: 'bow',\n",
        " 457: 'bow tie, bow-tie, bowtie',\n",
        " 458: 'brass, memorial tablet, plaque',\n",
        " 459: 'brassiere, bra, bandeau',\n",
        " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
        " 461: 'breastplate, aegis, egis',\n",
        " 462: 'broom',\n",
        " 463: 'bucket, pail',\n",
        " 464: 'buckle',\n",
        " 465: 'bulletproof vest',\n",
        " 466: 'bullet train, bullet',\n",
        " 467: 'butcher shop, meat market',\n",
        " 468: 'cab, hack, taxi, taxicab',\n",
        " 469: 'caldron, cauldron',\n",
        " 470: 'candle, taper, wax light',\n",
        " 471: 'cannon',\n",
        " 472: 'canoe',\n",
        " 473: 'can opener, tin opener',\n",
        " 474: 'cardigan',\n",
        " 475: 'car mirror',\n",
        " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
        " 477: \"carpenter's kit, tool kit\",\n",
        " 478: 'carton',\n",
        " 479: 'car wheel',\n",
        " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
        " 481: 'cassette',\n",
        " 482: 'cassette player',\n",
        " 483: 'castle',\n",
        " 484: 'catamaran',\n",
        " 485: 'CD player',\n",
        " 486: 'cello, violoncello',\n",
        " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
        " 488: 'chain',\n",
        " 489: 'chainlink fence',\n",
        " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
        " 491: 'chain saw, chainsaw',\n",
        " 492: 'chest',\n",
        " 493: 'chiffonier, commode',\n",
        " 494: 'chime, bell, gong',\n",
        " 495: 'china cabinet, china closet',\n",
        " 496: 'Christmas stocking',\n",
        " 497: 'church, church building',\n",
        " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
        " 499: 'cleaver, meat cleaver, chopper',\n",
        " 500: 'cliff dwelling',\n",
        " 501: 'cloak',\n",
        " 502: 'clog, geta, patten, sabot',\n",
        " 503: 'cocktail shaker',\n",
        " 504: 'coffee mug',\n",
        " 505: 'coffeepot',\n",
        " 506: 'coil, spiral, volute, whorl, helix',\n",
        " 507: 'combination lock',\n",
        " 508: 'computer keyboard, keypad',\n",
        " 509: 'confectionery, confectionary, candy store',\n",
        " 510: 'container ship, containership, container vessel',\n",
        " 511: 'convertible',\n",
        " 512: 'corkscrew, bottle screw',\n",
        " 513: 'cornet, horn, trumpet, trump',\n",
        " 514: 'cowboy boot',\n",
        " 515: 'cowboy hat, ten-gallon hat',\n",
        " 516: 'cradle',\n",
        " 517: 'crane',\n",
        " 518: 'crash helmet',\n",
        " 519: 'crate',\n",
        " 520: 'crib, cot',\n",
        " 521: 'Crock Pot',\n",
        " 522: 'croquet ball',\n",
        " 523: 'crutch',\n",
        " 524: 'cuirass',\n",
        " 525: 'dam, dike, dyke',\n",
        " 526: 'desk',\n",
        " 527: 'desktop computer',\n",
        " 528: 'dial telephone, dial phone',\n",
        " 529: 'diaper, nappy, napkin',\n",
        " 530: 'digital clock',\n",
        " 531: 'digital watch',\n",
        " 532: 'dining table, board',\n",
        " 533: 'dishrag, dishcloth',\n",
        " 534: 'dishwasher, dish washer, dishwashing machine',\n",
        " 535: 'disk brake, disc brake',\n",
        " 536: 'dock, dockage, docking facility',\n",
        " 537: 'dogsled, dog sled, dog sleigh',\n",
        " 538: 'dome',\n",
        " 539: 'doormat, welcome mat',\n",
        " 540: 'drilling platform, offshore rig',\n",
        " 541: 'drum, membranophone, tympan',\n",
        " 542: 'drumstick',\n",
        " 543: 'dumbbell',\n",
        " 544: 'Dutch oven',\n",
        " 545: 'electric fan, blower',\n",
        " 546: 'electric guitar',\n",
        " 547: 'electric locomotive',\n",
        " 548: 'entertainment center',\n",
        " 549: 'envelope',\n",
        " 550: 'espresso maker',\n",
        " 551: 'face powder',\n",
        " 552: 'feather boa, boa',\n",
        " 553: 'file, file cabinet, filing cabinet',\n",
        " 554: 'fireboat',\n",
        " 555: 'fire engine, fire truck',\n",
        " 556: 'fire screen, fireguard',\n",
        " 557: 'flagpole, flagstaff',\n",
        " 558: 'flute, transverse flute',\n",
        " 559: 'folding chair',\n",
        " 560: 'football helmet',\n",
        " 561: 'forklift',\n",
        " 562: 'fountain',\n",
        " 563: 'fountain pen',\n",
        " 564: 'four-poster',\n",
        " 565: 'freight car',\n",
        " 566: 'French horn, horn',\n",
        " 567: 'frying pan, frypan, skillet',\n",
        " 568: 'fur coat',\n",
        " 569: 'garbage truck, dustcart',\n",
        " 570: 'gasmask, respirator, gas helmet',\n",
        " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
        " 572: 'goblet',\n",
        " 573: 'go-kart',\n",
        " 574: 'golf ball',\n",
        " 575: 'golfcart, golf cart',\n",
        " 576: 'gondola',\n",
        " 577: 'gong, tam-tam',\n",
        " 578: 'gown',\n",
        " 579: 'grand piano, grand',\n",
        " 580: 'greenhouse, nursery, glasshouse',\n",
        " 581: 'grille, radiator grille',\n",
        " 582: 'grocery store, grocery, food market, market',\n",
        " 583: 'guillotine',\n",
        " 584: 'hair slide',\n",
        " 585: 'hair spray',\n",
        " 586: 'half track',\n",
        " 587: 'hammer',\n",
        " 588: 'hamper',\n",
        " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
        " 590: 'hand-held computer, hand-held microcomputer',\n",
        " 591: 'handkerchief, hankie, hanky, hankey',\n",
        " 592: 'hard disc, hard disk, fixed disk',\n",
        " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
        " 594: 'harp',\n",
        " 595: 'harvester, reaper',\n",
        " 596: 'hatchet',\n",
        " 597: 'holster',\n",
        " 598: 'home theater, home theatre',\n",
        " 599: 'honeycomb',\n",
        " 600: 'hook, claw',\n",
        " 601: 'hoopskirt, crinoline',\n",
        " 602: 'horizontal bar, high bar',\n",
        " 603: 'horse cart, horse-cart',\n",
        " 604: 'hourglass',\n",
        " 605: 'iPod',\n",
        " 606: 'iron, smoothing iron',\n",
        " 607: \"jack-o'-lantern\",\n",
        " 608: 'jean, blue jean, denim',\n",
        " 609: 'jeep, landrover',\n",
        " 610: 'jersey, T-shirt, tee shirt',\n",
        " 611: 'jigsaw puzzle',\n",
        " 612: 'jinrikisha, ricksha, rickshaw',\n",
        " 613: 'joystick',\n",
        " 614: 'kimono',\n",
        " 615: 'knee pad',\n",
        " 616: 'knot',\n",
        " 617: 'lab coat, laboratory coat',\n",
        " 618: 'ladle',\n",
        " 619: 'lampshade, lamp shade',\n",
        " 620: 'laptop, laptop computer',\n",
        " 621: 'lawn mower, mower',\n",
        " 622: 'lens cap, lens cover',\n",
        " 623: 'letter opener, paper knife, paperknife',\n",
        " 624: 'library',\n",
        " 625: 'lifeboat',\n",
        " 626: 'lighter, light, igniter, ignitor',\n",
        " 627: 'limousine, limo',\n",
        " 628: 'liner, ocean liner',\n",
        " 629: 'lipstick, lip rouge',\n",
        " 630: 'Loafer',\n",
        " 631: 'lotion',\n",
        " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
        " 633: \"loupe, jeweler's loupe\",\n",
        " 634: 'lumbermill, sawmill',\n",
        " 635: 'magnetic compass',\n",
        " 636: 'mailbag, postbag',\n",
        " 637: 'mailbox, letter box',\n",
        " 638: 'maillot',\n",
        " 639: 'maillot, tank suit',\n",
        " 640: 'manhole cover',\n",
        " 641: 'maraca',\n",
        " 642: 'marimba, xylophone',\n",
        " 643: 'mask',\n",
        " 644: 'matchstick',\n",
        " 645: 'maypole',\n",
        " 646: 'maze, labyrinth',\n",
        " 647: 'measuring cup',\n",
        " 648: 'medicine chest, medicine cabinet',\n",
        " 649: 'megalith, megalithic structure',\n",
        " 650: 'microphone, mike',\n",
        " 651: 'microwave, microwave oven',\n",
        " 652: 'military uniform',\n",
        " 653: 'milk can',\n",
        " 654: 'minibus',\n",
        " 655: 'miniskirt, mini',\n",
        " 656: 'minivan',\n",
        " 657: 'missile',\n",
        " 658: 'mitten',\n",
        " 659: 'mixing bowl',\n",
        " 660: 'mobile home, manufactured home',\n",
        " 661: 'Model T',\n",
        " 662: 'modem',\n",
        " 663: 'monastery',\n",
        " 664: 'monitor',\n",
        " 665: 'moped',\n",
        " 666: 'mortar',\n",
        " 667: 'mortarboard',\n",
        " 668: 'mosque',\n",
        " 669: 'mosquito net',\n",
        " 670: 'motor scooter, scooter',\n",
        " 671: 'mountain bike, all-terrain bike, off-roader',\n",
        " 672: 'mountain tent',\n",
        " 673: 'mouse, computer mouse',\n",
        " 674: 'mousetrap',\n",
        " 675: 'moving van',\n",
        " 676: 'muzzle',\n",
        " 677: 'nail',\n",
        " 678: 'neck brace',\n",
        " 679: 'necklace',\n",
        " 680: 'nipple',\n",
        " 681: 'notebook, notebook computer',\n",
        " 682: 'obelisk',\n",
        " 683: 'oboe, hautboy, hautbois',\n",
        " 684: 'ocarina, sweet potato',\n",
        " 685: 'odometer, hodometer, mileometer, milometer',\n",
        " 686: 'oil filter',\n",
        " 687: 'organ, pipe organ',\n",
        " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
        " 689: 'overskirt',\n",
        " 690: 'oxcart',\n",
        " 691: 'oxygen mask',\n",
        " 692: 'packet',\n",
        " 693: 'paddle, boat paddle',\n",
        " 694: 'paddlewheel, paddle wheel',\n",
        " 695: 'padlock',\n",
        " 696: 'paintbrush',\n",
        " 697: \"pajama, pyjama, pj's, jammies\",\n",
        " 698: 'palace',\n",
        " 699: 'panpipe, pandean pipe, syrinx',\n",
        " 700: 'paper towel',\n",
        " 701: 'parachute, chute',\n",
        " 702: 'parallel bars, bars',\n",
        " 703: 'park bench',\n",
        " 704: 'parking meter',\n",
        " 705: 'passenger car, coach, carriage',\n",
        " 706: 'patio, terrace',\n",
        " 707: 'pay-phone, pay-station',\n",
        " 708: 'pedestal, plinth, footstall',\n",
        " 709: 'pencil box, pencil case',\n",
        " 710: 'pencil sharpener',\n",
        " 711: 'perfume, essence',\n",
        " 712: 'Petri dish',\n",
        " 713: 'photocopier',\n",
        " 714: 'pick, plectrum, plectron',\n",
        " 715: 'pickelhaube',\n",
        " 716: 'picket fence, paling',\n",
        " 717: 'pickup, pickup truck',\n",
        " 718: 'pier',\n",
        " 719: 'piggy bank, penny bank',\n",
        " 720: 'pill bottle',\n",
        " 721: 'pillow',\n",
        " 722: 'ping-pong ball',\n",
        " 723: 'pinwheel',\n",
        " 724: 'pirate, pirate ship',\n",
        " 725: 'pitcher, ewer',\n",
        " 726: \"plane, carpenter's plane, woodworking plane\",\n",
        " 727: 'planetarium',\n",
        " 728: 'plastic bag',\n",
        " 729: 'plate rack',\n",
        " 730: 'plow, plough',\n",
        " 731: \"plunger, plumber's helper\",\n",
        " 732: 'Polaroid camera, Polaroid Land camera',\n",
        " 733: 'pole',\n",
        " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
        " 735: 'poncho',\n",
        " 736: 'pool table, billiard table, snooker table',\n",
        " 737: 'pop bottle, soda bottle',\n",
        " 738: 'pot, flowerpot',\n",
        " 739: \"potter's wheel\",\n",
        " 740: 'power drill',\n",
        " 741: 'prayer rug, prayer mat',\n",
        " 742: 'printer',\n",
        " 743: 'prison, prison house',\n",
        " 744: 'projectile, missile',\n",
        " 745: 'projector',\n",
        " 746: 'puck, hockey puck',\n",
        " 747: 'punching bag, punch bag, punching ball, punchball',\n",
        " 748: 'purse',\n",
        " 749: 'quill, quill pen',\n",
        " 750: 'quilt, comforter, comfort, puff',\n",
        " 751: 'racer, race car, racing car',\n",
        " 752: 'racket, racquet',\n",
        " 753: 'radiator',\n",
        " 754: 'radio, wireless',\n",
        " 755: 'radio telescope, radio reflector',\n",
        " 756: 'rain barrel',\n",
        " 757: 'recreational vehicle, RV, R.V.',\n",
        " 758: 'reel',\n",
        " 759: 'reflex camera',\n",
        " 760: 'refrigerator, icebox',\n",
        " 761: 'remote control, remote',\n",
        " 762: 'restaurant, eating house, eating place, eatery',\n",
        " 763: 'revolver, six-gun, six-shooter',\n",
        " 764: 'rifle',\n",
        " 765: 'rocking chair, rocker',\n",
        " 766: 'rotisserie',\n",
        " 767: 'rubber eraser, rubber, pencil eraser',\n",
        " 768: 'rugby ball',\n",
        " 769: 'rule, ruler',\n",
        " 770: 'running shoe',\n",
        " 771: 'safe',\n",
        " 772: 'safety pin',\n",
        " 773: 'saltshaker, salt shaker',\n",
        " 774: 'sandal',\n",
        " 775: 'sarong',\n",
        " 776: 'sax, saxophone',\n",
        " 777: 'scabbard',\n",
        " 778: 'scale, weighing machine',\n",
        " 779: 'school bus',\n",
        " 780: 'schooner',\n",
        " 781: 'scoreboard',\n",
        " 782: 'screen, CRT screen',\n",
        " 783: 'screw',\n",
        " 784: 'screwdriver',\n",
        " 785: 'seat belt, seatbelt',\n",
        " 786: 'sewing machine',\n",
        " 787: 'shield, buckler',\n",
        " 788: 'shoe shop, shoe-shop, shoe store',\n",
        " 789: 'shoji',\n",
        " 790: 'shopping basket',\n",
        " 791: 'shopping cart',\n",
        " 792: 'shovel',\n",
        " 793: 'shower cap',\n",
        " 794: 'shower curtain',\n",
        " 795: 'ski',\n",
        " 796: 'ski mask',\n",
        " 797: 'sleeping bag',\n",
        " 798: 'slide rule, slipstick',\n",
        " 799: 'sliding door',\n",
        " 800: 'slot, one-armed bandit',\n",
        " 801: 'snorkel',\n",
        " 802: 'snowmobile',\n",
        " 803: 'snowplow, snowplough',\n",
        " 804: 'soap dispenser',\n",
        " 805: 'soccer ball',\n",
        " 806: 'sock',\n",
        " 807: 'solar dish, solar collector, solar furnace',\n",
        " 808: 'sombrero',\n",
        " 809: 'soup bowl',\n",
        " 810: 'space bar',\n",
        " 811: 'space heater',\n",
        " 812: 'space shuttle',\n",
        " 813: 'spatula',\n",
        " 814: 'speedboat',\n",
        " 815: \"spider web, spider's web\",\n",
        " 816: 'spindle',\n",
        " 817: 'sports car, sport car',\n",
        " 818: 'spotlight, spot',\n",
        " 819: 'stage',\n",
        " 820: 'steam locomotive',\n",
        " 821: 'steel arch bridge',\n",
        " 822: 'steel drum',\n",
        " 823: 'stethoscope',\n",
        " 824: 'stole',\n",
        " 825: 'stone wall',\n",
        " 826: 'stopwatch, stop watch',\n",
        " 827: 'stove',\n",
        " 828: 'strainer',\n",
        " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
        " 830: 'stretcher',\n",
        " 831: 'studio couch, day bed',\n",
        " 832: 'stupa, tope',\n",
        " 833: 'submarine, pigboat, sub, U-boat',\n",
        " 834: 'suit, suit of clothes',\n",
        " 835: 'sundial',\n",
        " 836: 'sunglass',\n",
        " 837: 'sunglasses, dark glasses, shades',\n",
        " 838: 'sunscreen, sunblock, sun blocker',\n",
        " 839: 'suspension bridge',\n",
        " 840: 'swab, swob, mop',\n",
        " 841: 'sweatshirt',\n",
        " 842: 'swimming trunks, bathing trunks',\n",
        " 843: 'swing',\n",
        " 844: 'switch, electric switch, electrical switch',\n",
        " 845: 'syringe',\n",
        " 846: 'table lamp',\n",
        " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
        " 848: 'tape player',\n",
        " 849: 'teapot',\n",
        " 850: 'teddy, teddy bear',\n",
        " 851: 'television, television system',\n",
        " 852: 'tennis ball',\n",
        " 853: 'thatch, thatched roof',\n",
        " 854: 'theater curtain, theatre curtain',\n",
        " 855: 'thimble',\n",
        " 856: 'thresher, thrasher, threshing machine',\n",
        " 857: 'throne',\n",
        " 858: 'tile roof',\n",
        " 859: 'toaster',\n",
        " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
        " 861: 'toilet seat',\n",
        " 862: 'torch',\n",
        " 863: 'totem pole',\n",
        " 864: 'tow truck, tow car, wrecker',\n",
        " 865: 'toyshop',\n",
        " 866: 'tractor',\n",
        " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
        " 868: 'tray',\n",
        " 869: 'trench coat',\n",
        " 870: 'tricycle, trike, velocipede',\n",
        " 871: 'trimaran',\n",
        " 872: 'tripod',\n",
        " 873: 'triumphal arch',\n",
        " 874: 'trolleybus, trolley coach, trackless trolley',\n",
        " 875: 'trombone',\n",
        " 876: 'tub, vat',\n",
        " 877: 'turnstile',\n",
        " 878: 'typewriter keyboard',\n",
        " 879: 'umbrella',\n",
        " 880: 'unicycle, monocycle',\n",
        " 881: 'upright, upright piano',\n",
        " 882: 'vacuum, vacuum cleaner',\n",
        " 883: 'vase',\n",
        " 884: 'vault',\n",
        " 885: 'velvet',\n",
        " 886: 'vending machine',\n",
        " 887: 'vestment',\n",
        " 888: 'viaduct',\n",
        " 889: 'violin, fiddle',\n",
        " 890: 'volleyball',\n",
        " 891: 'waffle iron',\n",
        " 892: 'wall clock',\n",
        " 893: 'wallet, billfold, notecase, pocketbook',\n",
        " 894: 'wardrobe, closet, press',\n",
        " 895: 'warplane, military plane',\n",
        " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
        " 897: 'washer, automatic washer, washing machine',\n",
        " 898: 'water bottle',\n",
        " 899: 'water jug',\n",
        " 900: 'water tower',\n",
        " 901: 'whiskey jug',\n",
        " 902: 'whistle',\n",
        " 903: 'wig',\n",
        " 904: 'window screen',\n",
        " 905: 'window shade',\n",
        " 906: 'Windsor tie',\n",
        " 907: 'wine bottle',\n",
        " 908: 'wing',\n",
        " 909: 'wok',\n",
        " 910: 'wooden spoon',\n",
        " 911: 'wool, woolen, woollen',\n",
        " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
        " 913: 'wreck',\n",
        " 914: 'yawl',\n",
        " 915: 'yurt',\n",
        " 916: 'web site, website, internet site, site',\n",
        " 917: 'comic book',\n",
        " 918: 'crossword puzzle, crossword',\n",
        " 919: 'street sign',\n",
        " 920: 'traffic light, traffic signal, stoplight',\n",
        " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
        " 922: 'menu',\n",
        " 923: 'plate',\n",
        " 924: 'guacamole',\n",
        " 925: 'consomme',\n",
        " 926: 'hot pot, hotpot',\n",
        " 927: 'trifle',\n",
        " 928: 'ice cream, icecream',\n",
        " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
        " 930: 'French loaf',\n",
        " 931: 'bagel, beigel',\n",
        " 932: 'pretzel',\n",
        " 933: 'cheeseburger',\n",
        " 934: 'hotdog, hot dog, red hot',\n",
        " 935: 'mashed potato',\n",
        " 936: 'head cabbage',\n",
        " 937: 'broccoli',\n",
        " 938: 'cauliflower',\n",
        " 939: 'zucchini, courgette',\n",
        " 940: 'spaghetti squash',\n",
        " 941: 'acorn squash',\n",
        " 942: 'butternut squash',\n",
        " 943: 'cucumber, cuke',\n",
        " 944: 'artichoke, globe artichoke',\n",
        " 945: 'bell pepper',\n",
        " 946: 'cardoon',\n",
        " 947: 'mushroom',\n",
        " 948: 'Granny Smith',\n",
        " 949: 'strawberry',\n",
        " 950: 'orange',\n",
        " 951: 'lemon',\n",
        " 952: 'fig',\n",
        " 953: 'pineapple, ananas',\n",
        " 954: 'banana',\n",
        " 955: 'jackfruit, jak, jack',\n",
        " 956: 'custard apple',\n",
        " 957: 'pomegranate',\n",
        " 958: 'hay',\n",
        " 959: 'carbonara',\n",
        " 960: 'chocolate sauce, chocolate syrup',\n",
        " 961: 'dough',\n",
        " 962: 'meat loaf, meatloaf',\n",
        " 963: 'pizza, pizza pie',\n",
        " 964: 'potpie',\n",
        " 965: 'burrito',\n",
        " 966: 'red wine',\n",
        " 967: 'espresso',\n",
        " 968: 'cup',\n",
        " 969: 'eggnog',\n",
        " 970: 'alp',\n",
        " 971: 'bubble',\n",
        " 972: 'cliff, drop, drop-off',\n",
        " 973: 'coral reef',\n",
        " 974: 'geyser',\n",
        " 975: 'lakeside, lakeshore',\n",
        " 976: 'promontory, headland, head, foreland',\n",
        " 977: 'sandbar, sand bar',\n",
        " 978: 'seashore, coast, seacoast, sea-coast',\n",
        " 979: 'valley, vale',\n",
        " 980: 'volcano',\n",
        " 981: 'ballplayer, baseball player',\n",
        " 982: 'groom, bridegroom',\n",
        " 983: 'scuba diver',\n",
        " 984: 'rapeseed',\n",
        " 985: 'daisy',\n",
        " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
        " 987: 'corn',\n",
        " 988: 'acorn',\n",
        " 989: 'hip, rose hip, rosehip',\n",
        " 990: 'buckeye, horse chestnut, conker',\n",
        " 991: 'coral fungus',\n",
        " 992: 'agaric',\n",
        " 993: 'gyromitra',\n",
        " 994: 'stinkhorn, carrion fungus',\n",
        " 995: 'earthstar',\n",
        " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
        " 997: 'bolete',\n",
        " 998: 'ear, spike, capitulum',\n",
        " 999: 'toilet tissue, toilet paper, bathroom tissue'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-syEMmuArgv",
        "colab_type": "code",
        "outputId": "ead87f74-85e9-4b4a-a5eb-07f2681ac8c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "imagenet_classes = list(imagenet_classes_mapping.values())\n",
        "imagenet_classes[:10]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tench, Tinca tinca',\n",
              " 'goldfish, Carassius auratus',\n",
              " 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
              " 'tiger shark, Galeocerdo cuvieri',\n",
              " 'hammerhead, hammerhead shark',\n",
              " 'electric ray, crampfish, numbfish, torpedo',\n",
              " 'stingray',\n",
              " 'cock',\n",
              " 'hen',\n",
              " 'ostrich, Struthio camelus']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "30D0Scxk4kn3"
      },
      "source": [
        "# Libs import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwAuDtWDnIep",
        "colab_type": "code",
        "outputId": "f065a1fa-0375-4748-a7ea-e6da7b41e398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!pip install torch==1.4.0\n",
        "!pip install pytorch-pretrained-biggan\n",
        "!pip install flair"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-biggan in /usr/local/lib/python3.6/dist-packages (0.1.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-biggan) (1.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-biggan) (1.17.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-biggan) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-biggan) (4.28.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-biggan) (1.4.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-biggan) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-biggan) (1.14.15)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-biggan) (0.3.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-biggan) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-biggan) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-biggan) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-biggan) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-biggan) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-biggan) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-biggan) (1.12.0)\n",
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/29/81e3c9a829ec50857c23d82560941625f6b42ce76ee7c56ea9529e959d18/flair-0.4.5-py3-none-any.whl (136kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.4.0)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.1.3)\n",
            "Collecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 8.4MB/s \n",
            "\u001b[?25hCollecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/59/6ed78856ab99d2da04084b59e7da797972baa0efecb71546b16d48e49d9b/segtok-1.5.7.tar.gz\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Collecting pytest>=5.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/e2/c19c667f42f72716a7d03e8dd4d6f63f47d39feadd44cc1ee7ca3089862c/pytest-5.4.1-py3-none-any.whl (246kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 15.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (2.6.1)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.20)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/f6/89/62912e01f3cede11edcc0abf81298e3439d9c06c8dce644369380ed13f6d/Deprecated-1.2.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.28.1)\n",
            "Collecting transformers>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 12.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.22.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.6)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 18.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair) (1.24.3)\n",
            "Requirement already satisfied: bpemb>=0.2.9 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.6)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.17.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.4)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.10.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.9.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (0.1.8)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (8.2.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (19.3.0)\n",
            "Collecting pluggy<1.0,>=0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (20.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.8.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.11.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->flair) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 24.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->flair) (0.1.85)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->flair) (1.11.15)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 30.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->flair) (2.21.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair) (0.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.2.3->flair) (45.2.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.1)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair) (3.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.3.0->flair) (7.0)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.3.0->flair) (1.14.15)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.3.0->flair) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.3.0->flair) (0.9.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.3.0->flair) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.3.0->flair) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.3.0->flair) (3.0.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers>=2.3.0->flair) (0.15.2)\n",
            "Building wheels for collected packages: mpld3, segtok, sqlitedict, langdetect, sacremoses\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116679 sha256=ea4977541c82a1c3834ce545a1d482a385690c76c8387976ccc343a7234df23b\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.7-cp36-none-any.whl size=23258 sha256=8246e12ba6377f2626954eab3d11d284c6b823fa2d5e602acb6cbaff2d6aa4af\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/ee/a8/6112173f1386d33eebedb3f73429cfa41a4c3084556bcee254\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.6.0-cp36-none-any.whl size=14689 sha256=894d306ae016332cf22e6065d041b154899b59244fd4f609a96480cdeedd48b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993193 sha256=0a3e9af34f1e2e64b57aea7c223bcc6b5e3a5952335f35239dd3ef876aed8156\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=94c8a3f37be1c9774f258d2dc87f5bb0bd9b53a1df520bbc578d0e97cdc9ae09\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built mpld3 segtok sqlitedict langdetect sacremoses\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: mpld3, segtok, pluggy, pytest, sqlitedict, deprecated, sacremoses, tokenizers, transformers, langdetect, flair\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "Successfully installed deprecated-1.2.7 flair-0.4.5 langdetect-1.0.8 mpld3-0.3 pluggy-0.13.1 pytest-5.4.1 sacremoses-0.0.38 segtok-1.5.7 sqlitedict-1.6.0 tokenizers-0.5.2 transformers-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KvFQJkw9Vpu",
        "colab_type": "code",
        "outputId": "a73ed290-40af-4475-cc27-85ef23e1e903",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "!pip install bpemb;\n",
        "from bpemb import BPEmb"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bpemb\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bpemb) (1.17.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from bpemb) (3.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from bpemb) (4.28.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb) (2.21.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.9.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2.8)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bpemb) (1.11.15)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bpemb) (2.49.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (0.9.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->smart-open>=1.2.1->gensim->bpemb) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->smart-open>=1.2.1->gensim->bpemb) (2.6.1)\n",
            "Installing collected packages: sentencepiece, bpemb\n",
            "Successfully installed bpemb-0.3.0 sentencepiece-0.1.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab_type": "code",
        "id": "ZnXUdrxQ3TAN",
        "outputId": "9b573216-194a-4752-fb9a-d1b8f56651da",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "import os, csv, random, pickle, re, sys\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "from pytorch_pretrained_biggan import (BigGAN, one_hot_from_names, truncated_noise_sample,\n",
        "                                       save_as_images, display_in_terminal)\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# import tensorflow.keras.backend as K\n",
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n",
        "# from tensorflow.keras.layers import Activation, Dropout, CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, CuDNNGRU\n",
        "# from tensorflow.keras.callbacks import LearningRateScheduler, Callback, ModelCheckpoint\n",
        "\n",
        "import tensorflow as tf\n",
        "%tensorflow_version 1.x\n",
        "print(tf.__version__)\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.ERROR)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4.0\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ijseJsWUosmg"
      },
      "source": [
        "TODO: check if we can no to use this sess and use only default session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wi85ywI6MouM",
        "colab": {}
      },
      "source": [
        "# Initialize session\n",
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ypZqfsNI6-Ne"
      },
      "source": [
        "# Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3JtxOKtA35gA",
        "outputId": "d92cde08-28ac-4b5b-d01d-b78b9bb42770",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if tf.test.is_gpu_available(\n",
        "    cuda_only=False,\n",
        "    min_cuda_compute_capability=None\n",
        "):\n",
        "    print(tf.test.gpu_device_name())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VD2z8D2V7Dod"
      },
      "source": [
        "# Authorization on Google drive and configurings paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "okymGz_lEjLC",
        "colab": {}
      },
      "source": [
        "WORKSPACE = 'COLAB' # or 'KAGGLE'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "auzLnrdK51Rg",
        "outputId": "d4c2d709-9f53-4376-89bc-2ac09715293f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "if WORKSPACE == 'COLAB':\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    data_folder = os.path.join('/content/drive/My Drive', 'MLHack_2020')\n",
        "    google_pic_folder = os.path.join(data_folder, 'pictures_from_google')\n",
        "    output_folder = os.path.join(data_folder, 'GANS_output')\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "elif WORKSPACE == 'KAGGLE':\n",
        "    pass\n",
        "else: # TODO: add computing on premise\n",
        "    pass"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vPRZQr_7NKV",
        "colab_type": "text"
      },
      "source": [
        "#Parsing input files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAktwKprzzUf",
        "colab_type": "code",
        "outputId": "e3657645-0b72-4c82-bf9a-b3520436d9a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input_text_filename = \"request_from_frontend.txt\"\n",
        "# with open(os.path.join(data_folder, input_text_filename), \"w\") as text_file:\n",
        "#     text_file.write(\"human\")\n",
        "\n",
        "with open(os.path.join(data_folder, input_text_filename), \"r\") as text_file:\n",
        "    input_text = text_file.readline()\n",
        "\n",
        "words = len(input_text.split(' '))\n",
        "input_text, words"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('human', 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOkoFRAO9d5Q",
        "colab_type": "code",
        "outputId": "566eab9b-4828-4759-8642-e7eafcbf0104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# load English BPEmb model with default vocabulary size of max_tokens\n",
        "bpemb_en = BPEmb(lang=\"en\", dim=300, vs=50000)\n",
        "bpemb_en.vectors.shape"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs50000.model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1100587/1100587 [00:00<00:00, 1831159.11B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs50000.d300.w2v.bin.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 56053495/56053495 [00:03<00:00, 18031544.91B/s]\n",
            "INFO:gensim.models.utils_any2vec:loading projection weights from /root/.cache/bpemb/en/en.wiki.bpe.vs50000.d300.w2v.bin\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "INFO:gensim.models.utils_any2vec:loaded (50000, 300) matrix from /root/.cache/bpemb/en/en.wiki.bpe.vs50000.d300.w2v.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6DwCdXP-Cn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mean vector of word parts\n",
        "#works even if input_text is a sentence\n",
        "def get_vec(input_text):\n",
        "    vec = bpemb_en.embed(input_text)\n",
        "    return vec.mean(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt3HmzLl-iYo",
        "colab_type": "code",
        "outputId": "c3713b7a-0ac0-4193-b9b4-98b70eb251e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "similar_words = bpemb_en.most_similar(positive='dog'.split(' '), topn=10)\n",
        "similar_words[:10]"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:gensim.models.keyedvectors:precomputing L2-norms of word weight vectors\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('▁dog', 0.6598322987556458),\n",
              " ('dogs', 0.4702768325805664),\n",
              " ('▁dogs', 0.4588073492050171),\n",
              " ('hound', 0.42956024408340454),\n",
              " ('fight', 0.4138543903827667),\n",
              " ('▁breed', 0.3995840847492218),\n",
              " ('▁hound', 0.37506139278411865),\n",
              " ('ghost', 0.35911664366722107),\n",
              " ('▁whisper', 0.35347700119018555),\n",
              " ('▁wolf', 0.35041308403015137)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68Afr8jSCe9k",
        "colab_type": "code",
        "outputId": "0e99e301-0afe-4882-fc3c-4156e8d31064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "[imagenet_class for imagenet_class in imagenet_classes if any(word[0] in imagenet_class for word in similar_words)]"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Afghan hound, Afghan',\n",
              " 'basset, basset hound',\n",
              " 'bloodhound, sleuthhound',\n",
              " 'black-and-tan coonhound',\n",
              " 'Walker hound, Walker foxhound',\n",
              " 'English foxhound',\n",
              " 'borzoi, Russian wolfhound',\n",
              " 'Irish wolfhound',\n",
              " 'Italian greyhound',\n",
              " 'Ibizan hound, Ibizan Podenco',\n",
              " 'Norwegian elkhound, elkhound',\n",
              " 'otterhound, otter hound',\n",
              " 'Saluki, gazelle hound',\n",
              " 'Scottish deerhound, deerhound',\n",
              " 'dogsled, dog sled, dog sleigh']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMSor6RMRW5-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "bfdf92f9-214a-441b-b74a-cd7516733dee"
      },
      "source": [
        "for imagenet_class in imagenet_classes:\n",
        "    for word in similar_words:\n",
        "        if word[0] in imagenet_class.split(' '):\n",
        "            print('near word:', word[0], '\\nfound imagenet_class:', imagenet_class)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "near word: hound \n",
            "found imagenet_class: basset, basset hound\n",
            "near word: hound \n",
            "found imagenet_class: otterhound, otter hound\n",
            "near word: hound \n",
            "found imagenet_class: Saluki, gazelle hound\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMTCzBAzWZEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if words == 1:\n",
        "    USE_BIGGAN = True\n",
        "    request = preprocessed_word\n",
        "else:\n",
        "    USE_BIGGAN = False\n",
        "    request = get_google_images_Vlad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XF1jS6AXSOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "request = 'jeep'\n",
        "USE_BIGGAN = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHhagP6g7hbb",
        "colab_type": "text"
      },
      "source": [
        "#GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SHhSMjlYYHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vectors(request, batch_size):\n",
        "    class_vector = one_hot_from_names([request], batch_size=batch_size)\n",
        "    noise_vector = truncated_noise_sample(truncation=truncation, batch_size=batch_size)\n",
        "\n",
        "    # All in tensors\n",
        "    class_vector = torch.from_numpy(class_vector)\n",
        "    noise_vector = torch.from_numpy(noise_vector)\n",
        "    \n",
        "    # If you have a GPU, put everything on cuda\n",
        "    class_vector = class_vector.to('cuda')\n",
        "    noise_vector = noise_vector.to('cuda')\n",
        "\n",
        "    return class_vector, noise_vector\n",
        "\n",
        "def generate_and_save_image(class_vector, noise_vector, truncation, store_path=os.path.join(output_folder, 'image_from_GAN')):    \n",
        "    # Generate an image\n",
        "    with torch.no_grad():\n",
        "        output = model(noise_vector, class_vector, truncation)\n",
        "\n",
        "    # If you have a GPU put back on CPU\n",
        "    output = output.to('cpu')\n",
        "\n",
        "    # Save results as png images\n",
        "    save_as_images(output, store_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O6g6UBI0uqD",
        "colab_type": "code",
        "outputId": "90146bf4-07cf-4d69-85c2-8bbd370052fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if USE_BIGGAN:\n",
        "     # Load pre-trained model tokenizer (vocabulary)\n",
        "    model = BigGAN.from_pretrained('biggan-deep-512')\n",
        "    # If you have a GPU, put everything on cuda\n",
        "    model.to('cuda')\n",
        "    # Prepare an input\n",
        "    truncation = 0.4\n",
        "    batch_size = 1\n",
        "\n",
        "    class_vector, noise_vector = get_vectors(request, batch_size)\n",
        "\n",
        "    generate_and_save_image(class_vector, noise_vector, truncation)\n",
        "else:\n",
        "    pass"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_biggan.model:loading model biggan-deep-512 from cache at /root/.pytorch_pretrained_biggan/07b5c0d1791fa2028f8aa458a36360f31e1549d44152c96579b3ad6b35055d34.c33e135ad91e13528d0b83edc3c53c7bf94f620bdf8bc2bf08be82ba6d602e62\n",
            "INFO:pytorch_pretrained_biggan.model:Model config {\n",
            "  \"attention_layer_position\": 8,\n",
            "  \"channel_width\": 128,\n",
            "  \"class_embed_dim\": 128,\n",
            "  \"eps\": 0.0001,\n",
            "  \"layers\": [\n",
            "    [\n",
            "      false,\n",
            "      16,\n",
            "      16\n",
            "    ],\n",
            "    [\n",
            "      true,\n",
            "      16,\n",
            "      16\n",
            "    ],\n",
            "    [\n",
            "      false,\n",
            "      16,\n",
            "      16\n",
            "    ],\n",
            "    [\n",
            "      true,\n",
            "      16,\n",
            "      8\n",
            "    ],\n",
            "    [\n",
            "      false,\n",
            "      8,\n",
            "      8\n",
            "    ],\n",
            "    [\n",
            "      true,\n",
            "      8,\n",
            "      8\n",
            "    ],\n",
            "    [\n",
            "      false,\n",
            "      8,\n",
            "      8\n",
            "    ],\n",
            "    [\n",
            "      true,\n",
            "      8,\n",
            "      4\n",
            "    ],\n",
            "    [\n",
            "      false,\n",
            "      4,\n",
            "      4\n",
            "    ],\n",
            "    [\n",
            "      true,\n",
            "      4,\n",
            "      2\n",
            "    ],\n",
            "    [\n",
            "      false,\n",
            "      2,\n",
            "      2\n",
            "    ],\n",
            "    [\n",
            "      true,\n",
            "      2,\n",
            "      1\n",
            "    ],\n",
            "    [\n",
            "      false,\n",
            "      1,\n",
            "      1\n",
            "    ],\n",
            "    [\n",
            "      true,\n",
            "      1,\n",
            "      1\n",
            "    ]\n",
            "  ],\n",
            "  \"n_stats\": 51,\n",
            "  \"num_classes\": 1000,\n",
            "  \"output_dim\": 512,\n",
            "  \"z_dim\": 128\n",
            "}\n",
            "\n",
            "INFO:pytorch_pretrained_biggan.utils:Saving image to /content/drive/My Drive/MLHack_2020/GANS_output/image_from_GAN_0.png\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYs3R5I9Ot2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eyxmTYb4Q2dp"
      },
      "source": [
        "# BERT fine-tuned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CE-uRZUmQ4d9",
        "outputId": "57380b37-23c3-434d-cbc7-155e3bca7f98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "print(hub.__version__)\n",
        "\n",
        "#Installing BERT module\n",
        "!pip install bert-tensorflow\n",
        "\n",
        "#Importing BERT modules\n",
        "import bert\n",
        "from bert import run_classifier, optimization, tokenization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7.0\n",
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XTP1fY-25QaB",
        "colab": {}
      },
      "source": [
        "def create_examples(df, labels_available=True):\n",
        "    \"\"\"\n",
        "    Creates input examples for the sets of texts and labels.\n",
        "    https://github.com/google-research/bert/blob/cc7051dc592802f501e8a6f71f8fb3cf9de95dc9/run_classifier.py#L127    \n",
        "    \"\"\"\n",
        "    examples = []\n",
        "    for (i, row) in enumerate(df.values):\n",
        "        guid = None  #row[0]\n",
        "        text_a = row[0]\n",
        "        if labels_available:\n",
        "            labels = row[1:].tolist()\n",
        "        else:  # what should be here in test phase\n",
        "            labels = [0, 0, 0, 0, 0, 0]\n",
        "        examples.append(\n",
        "            bert.run_classifier.InputExample(guid=guid,\n",
        "                                             text_a=text_a,\n",
        "                                             label=labels))\n",
        "    return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iMiU0RrN77JS",
        "outputId": "d5f508c6-e3c0-4c71-a1b1-98acf9cfcf1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\n",
        "\n",
        "\n",
        "def create_tokenizer_from_hub_module(bert_path):\n",
        "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "    bert_module = hub.Module(bert_path)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\",\n",
        "                                    as_dict=True)\n",
        "    vocab_file, do_lower_case = sess.run([\n",
        "        tokenization_info[\"vocab_file\"],\n",
        "        tokenization_info[\"do_lower_case\"],\n",
        "    ])\n",
        "    print('loading tokenizer with{} lowercasing'.format('out' *\n",
        "                                                        (not do_lower_case)))\n",
        "\n",
        "    return bert.tokenization.FullTokenizer(vocab_file=vocab_file,\n",
        "                                           do_lower_case=do_lower_case)\n",
        "\n",
        "\n",
        "bert_tokenizer = create_tokenizer_from_hub_module(BERT_MODEL_HUB)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loading tokenizer without lowercasing\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rhlik1IGTLfu",
        "colab": {}
      },
      "source": [
        "#todo: split to funct like here https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22\n",
        "class InputFeatures(object):\n",
        "    \"\"\"\n",
        "    A single set of features of data.\n",
        "    https://github.com/google-research/bert/blob/cc7051dc592802f501e8a6f71f8fb3cf9de95dc9/run_classifier.py#L161\n",
        "    The only difference from the original implementation (line above) is label_ids instead of label_id because\n",
        "    here we solve multi-label not multi-class task.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 input_ids,\n",
        "                 input_mask,\n",
        "                 segment_ids,\n",
        "                 label_ids,\n",
        "                 is_real_example=True):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_ids = label_ids,\n",
        "        self.is_real_example = is_real_example\n",
        "\n",
        "\n",
        "# split like here\n",
        "# https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22\n",
        "def convert_examples_to_features(examples, max_seq_length, tokenizer):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in tqdm(enumerate(examples), total=len(examples)):\n",
        "\n",
        "        #print(example.text_a)\n",
        "        tokens_a = tokenizer.tokenize(example.text_a)\n",
        "        tokens_b = None\n",
        "        if example.text_b:\n",
        "            tokens_b = tokenizer.tokenize(example.text_b)\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > max_seq_length - 2:\n",
        "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids: 0   0   0   0  0     0 0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        \"\"\"\n",
        "        # in original repo\n",
        "        tokens = []\n",
        "        segment_ids = []\n",
        "        tokens.append(\"[CLS]\")\n",
        "        segment_ids.append(0)\n",
        "        for token in tokens_a:\n",
        "            tokens.append(token)\n",
        "            segment_ids.append(0)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        segment_ids.append(0)\n",
        "        \"\"\"\n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
        "        segment_ids = [0] * len(tokens)\n",
        "        \"\"\"\n",
        "        # in original repo\n",
        "        if tokens_b:\n",
        "        for token in tokens_b:\n",
        "            tokens.append(token)\n",
        "            segment_ids.append(1)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        segment_ids.append(1)\n",
        "        \"\"\"\n",
        "        if tokens_b:\n",
        "            tokens += tokens_b + [\"[SEP]\"]\n",
        "            segment_ids += [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        '''\n",
        "        # in original repo\n",
        "        while len(input_ids) < max_seq_length:\n",
        "          input_ids.append(0)\n",
        "          input_mask.append(0)\n",
        "          segment_ids.append(0)\n",
        "        '''\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        labels_ids = []\n",
        "        for label in example.label:\n",
        "            labels_ids.append(int(label))\n",
        "\n",
        "        if ex_index < 2:  # make as param with default value\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % (example.guid))\n",
        "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x)\n",
        "                                                    for x in input_ids]))\n",
        "            logger.info(\"input_mask: %s\" %\n",
        "                        \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\"segment_ids: %s\" %\n",
        "                        \" \".join([str(x) for x in segment_ids]))\n",
        "            logger.info(\"label: %s (id = %s)\" % (example.label, labels_ids))\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(input_ids=input_ids,\n",
        "                          input_mask=input_mask,\n",
        "                          segment_ids=segment_ids,\n",
        "                          label_ids=labels_ids))\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tXl9kZi-wuXx",
        "colab": {}
      },
      "source": [
        "def features_to_arrays(features):\n",
        "    \"\"\"Convert a list of InputFeatures to np.arrays\"\"\"\n",
        "\n",
        "    all_input_ids = []\n",
        "    all_input_mask = []\n",
        "    all_segment_ids = []\n",
        "\n",
        "    for feature in features:\n",
        "        all_input_ids.append(feature.input_ids)\n",
        "        all_input_mask.append(feature.input_mask)\n",
        "        all_segment_ids.append(feature.segment_ids)\n",
        "\n",
        "    return (np.array(all_input_ids, dtype='int32'), \n",
        "            np.array(all_input_mask, dtype='int32'), \n",
        "            np.array(all_segment_ids, dtype='int32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9xcUsR46o-ou",
        "colab": {}
      },
      "source": [
        "def text2input_arrays(text, tokenizer, max_seq_length, labels_available, del_text=True):\n",
        "    examples = create_examples(text, labels_available=labels_available)\n",
        "    features = convert_examples_to_features(examples,  max_seq_length, tokenizer)\n",
        "    input_ids, input_masks, segment_ids = features_to_arrays(features)\n",
        "\n",
        "    if del_text:\n",
        "        del text\n",
        "        gc.collect()\n",
        "\n",
        "    return input_ids, input_masks, segment_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SMoU4oQJW8yi",
        "outputId": "206e5b49-29ae-4b2b-ea7a-96c75f14e8bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "preprocessed_train_with_labels = pd.concat([\n",
        "    pd.DataFrame(preprocessed_train, columns=['preprocessed_text']),\n",
        "    pd.DataFrame(train_labels, columns=TARGET_COLS)\n",
        "], axis=1)\n",
        "preprocessed_train_with_labels.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preprocessed_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>explanation why edits made username hardcore m...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>d'aww ! he matches background colour i'm seemi...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   preprocessed_text  ...  identity_hate\n",
              "0  explanation why edits made username hardcore m...  ...              0\n",
              "1  d'aww ! he matches background colour i'm seemi...  ...              0\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCCVkW6TeBN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del preprocessed_train\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q-2wIb1Ns-uv",
        "outputId": "d8052623-d9ed-44b5-b52d-0bc5f6e9e46b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "f0861523d94e4b9d9ce9d79ef6473755",
            "64565fdc2a104d078fa71080b85538fe",
            "47817ee2a74a42c3a8fc7204c0365f07",
            "b341646c4d5149f1aae165d71fc16340",
            "1604364b009f406a8833847cbc30a22a",
            "4fcaa74f5b29413bb2178c8520dd9f47",
            "113ef6ba71ce478e9d9e76f25c2c80ed",
            "c58927dabebb4816a2867a235c954809"
          ]
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "train_input_ids, train_input_masks, train_segment_ids = text2input_arrays(\n",
        "    text=preprocessed_train_with_labels,\n",
        "    tokenizer=bert_tokenizer,\n",
        "    max_seq_length=MAX_WORDS_IN_COMMENT,\n",
        "    labels_available=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0861523d94e4b9d9ce9d79ef6473755",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=159571), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "CPU times: user 2min 19s, sys: 1.57 s, total: 2min 20s\n",
            "Wall time: 2min 20s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EDqqF7PZ7yrP",
        "colab": {}
      },
      "source": [
        "train_input_ids, valid_input_ids, \\\n",
        "train_input_masks, valid_input_masks, \\\n",
        "train_segment_ids, valid_segment_ids, \\\n",
        "train_labels, valid_labels = \\\n",
        "train_test_split(train_input_ids, train_input_masks, train_segment_ids, train_labels, test_size = 0.1, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WLJDThTwf_Vb"
      },
      "source": [
        "TODO: create pipeline which uses features as is or without data->features->arrays but data->arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YF8fnd_OMGPX",
        "colab": {}
      },
      "source": [
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 bert_path,\n",
        "                 verbose=True,\n",
        "                 n_fine_tune_layers=10,\n",
        "                 **kwargs):\n",
        "        self.bert_path = bert_path\n",
        "        self.n_fine_tune_layers = n_fine_tune_layers\n",
        "        self.trainable = True\n",
        "        # parse it with regular expression\n",
        "        self.output_size = 768\n",
        "        self.verbose = verbose\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bert = hub.Module(self.bert_path,\n",
        "                               trainable=self.trainable,\n",
        "                               name=\"{}_module\".format(self.name))\n",
        "        trainable_vars = self.bert.variables\n",
        "\n",
        "        # Remove unused layers\n",
        "        trainable_vars = [\n",
        "            var for var in trainable_vars if not \"/cls/\" in var.name\n",
        "        ]\n",
        "\n",
        "        # Select how many layers to fine tune\n",
        "        trainable_vars = trainable_vars[-self.n_fine_tune_layers:]\n",
        "\n",
        "        # Add to trainable weights\n",
        "        for var in trainable_vars:\n",
        "            self._trainable_weights.append(var)\n",
        "\n",
        "        # Add non-trainable weights\n",
        "        for var in self.bert.variables:\n",
        "            if var not in self._trainable_weights:\n",
        "                self._non_trainable_weights.append(var)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"*** TRAINABLE VARS *** \")\n",
        "            for var in self._trainable_weights:\n",
        "                print(var)\n",
        "\n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        #inputs = [inputs.input_ids, inputs.input_mask, inputs.segment_ids]\n",
        "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        input_ids, input_mask, segment_ids = inputs\n",
        "        bert_inputs = dict(input_ids=input_ids,\n",
        "                           input_mask=input_mask,\n",
        "                           segment_ids=segment_ids)\n",
        "        result = self.bert(inputs=bert_inputs,\n",
        "                           signature=\"tokens\",\n",
        "                           as_dict=True)[\"pooled_output\"]\n",
        "        \"\"\"\n",
        "        output = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "        \n",
        "        if self.pooling == \"cls\":\n",
        "            pooled = output[\"pooled_output\"]\n",
        "        else:\n",
        "            result = output[\"sequence_output\"]\n",
        "            \n",
        "            input_mask = tf.cast(input_mask, tf.float32)\n",
        "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
        "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
        "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
        "            \n",
        "            if self.pooling == \"mean\":\n",
        "              pooled = masked_reduce_mean(result, input_mask)\n",
        "            else:\n",
        "              pooled = mul_mask(result, input_mask)\n",
        "\n",
        "        return pooled\n",
        "        \"\"\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_size)\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'bert_path': self.bert_path,\n",
        "            'verbose': self.verbose,\n",
        "            'n_fine_tune_layers': self.n_fine_tune_layers,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "# can take more fully model from https://github.com/strongio/keras-bert/blob/master/keras-bert.ipynb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9D8BuKRaMmiD",
        "colab": {}
      },
      "source": [
        "# Build model\n",
        "def build_model(max_seq_length): \n",
        "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
        "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
        "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
        "    bert_inputs = [in_id, in_mask, in_segment]\n",
        "    \n",
        "    # Instantiate the custom Bert Layer defined above\n",
        "    bert_output = BertLayer(BERT_MODEL_HUB, verbose=True, n_fine_tune_layers=5)(bert_inputs)\n",
        "\n",
        "    # Build the rest of the classifier \n",
        "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
        "    pred = tf.keras.layers.Dense(len(TARGET_COLS), activation='sigmoid')(dense)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "def initialize_vars(sess):\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    K.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QNCf8ZpEsQcL"
      },
      "source": [
        "TODO: clear memory and increase batchsize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q2XYJg0T07T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RocAucES = RocAucEarlyStopping(validation_data=([valid_input_ids, valid_input_masks, valid_segment_ids], valid_labels), \n",
        "                                   patience=1, digits=4)\n",
        "RocAucCheckpointer = ModelRocAucCheckpoint(filepath=output_folder+'/BertModel_multilabel.h5',\n",
        "                                           validation_data=([valid_input_ids, valid_input_masks, valid_segment_ids], valid_labels),\n",
        "                                            save_best_only=True,\n",
        "                                            save_weights_only=False,\n",
        "                                            digits=3,\n",
        "                                            save_freq=1,\n",
        "                                            verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tiH9J6v7IsZV",
        "outputId": "0460f7ad-ab81-47fe-c93a-f6f9b6f057ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gc.collect()\n",
        "\n",
        "model = build_model(max_seq_length=MAX_WORDS_IN_COMMENT)\n",
        "\n",
        "# Instantiate variables\n",
        "initialize_vars(sess)\n",
        "\n",
        "model.fit(\n",
        "    [train_input_ids, train_input_masks, train_segment_ids], \n",
        "    train_labels,\n",
        "    validation_data=([valid_input_ids, valid_input_masks, valid_segment_ids], valid_labels),\n",
        "    epochs=10,\n",
        "    batch_size=256,\n",
        "    callbacks=[RocAucES, RocAucCheckpointer]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** TRAINABLE VARS *** \n",
            "<tf.Variable 'bert_layer_module/bert/encoder/layer_9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "<tf.Variable 'bert_layer_module/bert/encoder/layer_9/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "<tf.Variable 'bert_layer_module/bert/encoder/layer_9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
            "<tf.Variable 'bert_layer_module/bert/pooler/dense/bias:0' shape=(768,) dtype=float32>\n",
            "<tf.Variable 'bert_layer_module/bert/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 220)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 220)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 220)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_layer (BertLayer)          (None, 768)          108931396   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          196864      bert_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 6)            1542        dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 109,129,802\n",
            "Trainable params: 3,149,830\n",
            "Non-trainable params: 105,979,972\n",
            "__________________________________________________________________________________________________\n",
            "Train on 143613 samples, validate on 15958 samples\n",
            "Epoch 1/10\n",
            "143360/143613 [============================>.] - ETA: 2s - loss: 0.0613\n",
            "ROC-AUC - epoch: 1 - score: 0.9804\n",
            "\n",
            "Epoch 00001: Roc Auc score improved from -inf to 0.98000, saving model to /content/drive/My Drive/Advanced NLP/Homework 1: Classical classification task like Kaggle Toxic or Quora/output/BertModel_multilabel.h5\n",
            "143613/143613 [==============================] - 1803s 13ms/sample - loss: 0.0612 - val_loss: 0.0500\n",
            "Epoch 2/10\n",
            "143360/143613 [============================>.] - ETA: 2s - loss: 0.0476\n",
            "ROC-AUC - epoch: 2 - score: 0.9851\n",
            "\n",
            "Epoch 00002: Roc Auc score improved from 0.98000 to 0.98500, saving model to /content/drive/My Drive/Advanced NLP/Homework 1: Classical classification task like Kaggle Toxic or Quora/output/BertModel_multilabel.h5\n",
            "143613/143613 [==============================] - 1793s 12ms/sample - loss: 0.0476 - val_loss: 0.0477\n",
            "Epoch 3/10\n",
            "143360/143613 [============================>.] - ETA: 2s - loss: 0.0430\n",
            "ROC-AUC - epoch: 3 - score: 0.9852\n",
            "\n",
            "Epoch 00003: Roc Auc score did not improve from 0.98500\n",
            "143613/143613 [==============================] - 1791s 12ms/sample - loss: 0.0430 - val_loss: 0.0529\n",
            "Epoch 4/10\n",
            "143360/143613 [============================>.] - ETA: 2s - loss: 0.0385\n",
            "ROC-AUC - epoch: 4 - score: 0.9862\n",
            "\n",
            "Epoch 00004: Roc Auc score improved from 0.98500 to 0.98600, saving model to /content/drive/My Drive/Advanced NLP/Homework 1: Classical classification task like Kaggle Toxic or Quora/output/BertModel_multilabel.h5\n",
            "143613/143613 [==============================] - 1793s 12ms/sample - loss: 0.0386 - val_loss: 0.0471\n",
            "Epoch 5/10\n",
            "143360/143613 [============================>.] - ETA: 2s - loss: 0.0342\n",
            "ROC-AUC - epoch: 5 - score: 0.9848\n",
            "\n",
            "best:0.9862\n",
            "current:0.9848\n",
            "0 patience remained\n",
            "\n",
            "Epoch 00005: Roc Auc score did not improve from 0.98600\n",
            "143613/143613 [==============================] - 1791s 12ms/sample - loss: 0.0342 - val_loss: 0.0489\n",
            "Epoch 6/10\n",
            "143360/143613 [============================>.] - ETA: 2s - loss: 0.0298\n",
            "ROC-AUC - epoch: 6 - score: 0.9843\n",
            "\n",
            "best:0.9862\n",
            "current:0.9843\n",
            "Early stopping due to lower roc auc\n",
            "\n",
            "Epoch 00006: Roc Auc score did not improve from 0.98600\n",
            "143613/143613 [==============================] - 1791s 12ms/sample - loss: 0.0298 - val_loss: 0.0525\n",
            "Epoch 7/10\n",
            "143360/143613 [============================>.] - ETA: 2s - loss: 0.0260\n",
            "ROC-AUC - epoch: 7 - score: 0.9839\n",
            "\n",
            "best:0.9862\n",
            "current:0.9839\n",
            "0 patience remained\n",
            "\n",
            "Epoch 00007: Roc Auc score did not improve from 0.98600\n",
            "143613/143613 [==============================] - 1791s 12ms/sample - loss: 0.0260 - val_loss: 0.0550\n",
            "Epoch 8/10\n",
            "  3584/143613 [..............................] - ETA: 22:48 - loss: 0.0217"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-6478ee7cb2c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRocAucES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRocAucCheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "imo2XJzmTxY7",
        "outputId": "087bb677-9459-46c0-8fe4-08325dd57e33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# Clear and load model\n",
        "del model\n",
        "gc.collect()\n",
        "model = build_model(MAX_WORDS_IN_COMMENT)\n",
        "initialize_vars(sess)\n",
        "model.load_weights(output_folder+'/BertModel_multilabel.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** TRAINABLE VARS *** \n",
            "<tf.Variable 'bert_layer_2_module/bert/encoder/layer_9/output/LayerNorm/gamma:0' shape=(768,) dtype=float32>\n",
            "<tf.Variable 'bert_layer_2_module/bert/encoder/layer_9/output/dense/bias:0' shape=(768,) dtype=float32>\n",
            "<tf.Variable 'bert_layer_2_module/bert/encoder/layer_9/output/dense/kernel:0' shape=(3072, 768) dtype=float32>\n",
            "<tf.Variable 'bert_layer_2_module/bert/pooler/dense/bias:0' shape=(768,) dtype=float32>\n",
            "<tf.Variable 'bert_layer_2_module/bert/pooler/dense/kernel:0' shape=(768, 768) dtype=float32>\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 220)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 220)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 220)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_layer_2 (BertLayer)        (None, 768)          108931396   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 256)          196864      bert_layer_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 6)            1542        dense_4[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 109,129,802\n",
            "Trainable params: 3,149,830\n",
            "Non-trainable params: 105,979,972\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V21lhkWODcfn",
        "outputId": "767711d0-37cf-4097-c403-e6e57d25aebe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "df8015745cbd4c30a12dfd44b273c963",
            "aa10d009e0ae4071bed574aa48f8771e",
            "20c9965e717c4089b4b4ad63952c0a1b",
            "4b64050383b745118540aab5408979cd",
            "e7ee05fa61b84b99b31beab29f4a8407",
            "cdf14fcb841c473f957636d853ddaa5e",
            "adffb40a3a9046929e59624559c6e72c",
            "cf3560801f8b41f0a45d2c475c7a2724"
          ]
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "preprocessed_test_df = pd.DataFrame(preprocessed_test,\n",
        "                                    columns=['preprocessed_text'])\n",
        "test_input_ids, test_input_masks, test_segment_ids = text2input_arrays(\n",
        "    text=preprocessed_test_df,\n",
        "    tokenizer=bert_tokenizer,\n",
        "    max_seq_length=MAX_WORDS_IN_COMMENT,\n",
        "    labels_available=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df8015745cbd4c30a12dfd44b273c963",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=153164), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "CPU times: user 2min 13s, sys: 0 ns, total: 2min 13s\n",
            "Wall time: 2min 12s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Odnkbt-ZnIj6",
        "outputId": "8cd1b91c-3959-49b4-aa6b-de90fc87ac90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "pred  = model.predict([test_input_ids, test_input_masks, test_segment_ids])\n",
        "pred.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 13min 26s, sys: 9min 42s, total: 23min 8s\n",
            "Wall time: 20min 49s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}